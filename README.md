# LB-PR-4-Hurzidze-Anton

## Хурцидзе Антон IПЗ 4.02 Лабораторна-Практична робота № 4

## Тема: Дослідження методу Градієнтного Спуску (ГС) для розв'язання задачі мультилатерації (TDoA) методом Найменших Квадратів (МНК).
## Мета: Практично реалізувати та дослідити роботу алгоритму градієнтного спуску. Зрозуміти суть Методу Найменших Квадратів (МНК) як способу формулювання функції втрат (loss function). Візуалізувати та проаналізувати ключові параметри оптимізації: швидкість навчання (learning rate), вплив шуму (noise level) та початкове припущення (initial guess). Дослідити інженерні трейдофи та фундаментальні проблеми реальних систем, такі як Геометричне Послаблення Точності (GDOP).

## 1. Ознайомлення з кодом

#### Відкриваємо наданий шаблон Google Colab за посиланням ``https://colab.research.google.com/drive/1CycvCPJSXxnZ7RvxvHCVIU34r_duzVXd?usp=sharing`` та ознайомимось с кодом та його структурою для виконання подальших досліджень:

![1](https://github.com/GAMECHl/LB-PR-4/blob/main/2.png)
#### Рис. 1 - Шаблон Google Colab

## 2. Експериментальні Завдання

### 1) Контрольний Запуск ("Ідеальний світ"):
#### Встановлюємо наступні налаштування:
``NOISE_LEVEL = 0 (повна відсутність шуму)
LEARNING_RATE = 0.01
INITIAL_GUESS = [50000, 50000]
TRUE_POSITION = [45000, 35000]``


![2](https://github.com/GAMECHl/LB-PR-4/blob/main/2.png)
#### Рис. 2 - Встановлені налаштування

#### Після виставлення налаштувань запускаємо алгоритм та дивимось на результат:

![3](https://github.com/GAMECHl/LB-PR-4/blob/main/3.png)
#### Рис. 3 - Графік алгоритму №1

### Аналіз та контрольні питання:
#### Питання 1:Запишіть фінальну похибку позиціонування. Чому вона не дорівнює 0.00 метрів, навіть за відсутності шуму? (Підказка: подивіться на TOLERANCE та графік "Delta Loss").
#### Питання 2: Проаналізуйте графік "Величина Градієнта". Як його поведінка пов'язана з графіком "Крива навчання"?
#### Відповідь 1: Похибка не 0.00 м через умову зупинки TOLERANCE. Алгоритм зупиняється, коли зміна Loss стає меншою за TOLERANCE, а не коли досягається абсолютна точність.
#### Відповідь 2: Величина градієнта показує "крутизну схилу". Спочатку великий градієнт → швидке падіння Loss. Коли наближаємось до мінімуму → градієнт зменшується → Loss падає повільніше.

### 2) Дослідження learning_rate ("Проблема Золотоволоски"):
#### Встановлюємо наступні налаштування:
``NOISE_LEVEL = 1e-6 (повна відсутність шуму)
Експеримент 2a (Розбіжність):  LEARNING_RATE = 2.0  Експеримент 2b (Стагнація):  LEARNING_RATE = 1e-5 Експеримент 2c (Оптимальний): LEARNING_RATE = 0.01 ``


![4](https://github.com/GAMECHl/LB-PR-4/blob/main/4.png)
#### Рис. 4 - Встановлені налаштування

#### Після виставлення налаштувань запускаємо алгоритм та дивимось на результат, потім послідовно змінюємо LEARNING_RATE:

![5](https://github.com/GAMECHl/LB-PR-4/blob/main/5.png)
#### Рис. 5 - Графік алгоритму №2а

![6](https://github.com/GAMECHl/LB-PR-4/blob/main/6.png)
#### Рис. 6 - Графік алгоритму №2b

![7](https://github.com/GAMECHl/LB-PR-4/blob/main/7.png)
#### Рис. 7 - Графік алгоритму №2c

### Аналіз та контрольні питання:
#### Питання 1: 2a: Що сталося з Loss? Проаналізуйте 2D-графік шляху. Поясніть термін "розбіжність" (divergence), спираючись на отримані графіки.
#### Питання 2: Для 2b: Чи зійшовся алгоритм за MAX_ITERATIONS? Порівняйте його криву навчання з 2c. Поясніть термін "стагнація" (stagnation).
#### Питання 3: Чому learning_rate є найважливішим гіперпараметром градієнтного спуску?
#### Відповідь 1: Loss хаотично стрибає вгору-вниз. На 2D-графіку шлях робить величезні петлі, відлітаючи на мільйони метрів від цілі. Розбіжність = занадто великі кроки викидають алгоритм у випадкових напрямках, стабільна збіжність неможлива.
#### Відповідь 2: Ні, не зійшовся за MAX_ITERATIONS. Крива навчання дуже пологa порівняно з 2c — Loss падає мізерно повільно. Стагнація = занадто малі кроки, потрібні десятки тисяч ітерацій для досягнення того ж результату.
#### Відповідь 3: Learning rate контролює розмір кроку. Занадто великий → хаос і розбіжність, занадто малий → марна трата обчислювальних ресурсів. Це баланс між швидкістю та стабільністю.

### 3) Дослідження NOISE_LEVEL ("Сміття на вході — сміття на виході")
#### Встановлюємо наступні налаштування:
``Експеримент 3a (Високоточний): NOISE_LEVEL = 1e-9 (1 нс)
Експеримент 3b (Реалістичний): NOISE_LEVEL = 1e-6 (1 мкс)
Експеримент 3c (Зламаний): NOISE_LEVEL = 1e-4 (100 мкс)``

#### Після виставлення налаштувань запускаємо алгоритм та дивимось на результат, потім послідовно змінюємо NOISE_LEVEL:

![8](https://github.com/GAMECHl/LB-PR-4/blob/main/8.png)
#### Рис. 8 - Графік алгоритму №3а

![9](https://github.com/GAMECHl/LB-PR-4/blob/main/9.png)
#### Рис. 9 - Графік алгоритму №3b

![10](https://github.com/GAMECHl/LB-PR-4/blob/main/10.png)
#### Рис. 10 - Графік алгоритму №3c

### Аналіз та контрольні питання:
#### Питання 1: Побудуйте таблицю: NOISE_LEVEL (в секундах) | Noise (в метрах) | Фінальна Похибка (в метрах).
#### Питання 2: Поясніть, чому в експерименті 3c алгоритм все одно сходиться (графік Loss падає), але фінальна похибка позиціонування є катастрофічною? До якого мінімуму він збігся?
#### Відповідь 1: Таблиця:
![11](https://github.com/GAMECHl/LB-PR-4/blob/main/11.png)
#### Відповідь 2: Алгоритм мінімізує квадратичну похибку від зашумлених вимірювань. Він збігся до "найкращого" рішення для цих неправильних даних. Це локальний мінімум зашумленої функції, а не справжня позиція.

### 4) Дослідження INITIAL_GUESS ("Вплив початкової точки")
#### Встановлюємо наступні налаштування:
``Експеримент 4a (Добре): INITIAL_GUESS = [50000, 50000]
Експеримент 4b (Погане): INITIAL_GUESS = [0, 0]
Експеримент 4c (Дуже погане): INITIAL_GUESS = [-500000, -500000]``

#### Після виставлення налаштувань запускаємо алгоритм та дивимось на результат, потім послідовно змінюємо INITIAL_GUESS:

![12](https://github.com/GAMECHl/LB-PR-4/blob/main/12.png)
#### Рис. 12 - Графік алгоритму №4а

![13](https://github.com/GAMECHl/LB-PR-4/blob/main/13.png)
#### Рис. 13 - Графік алгоритму №4b

![14](https://github.com/GAMECHl/LB-PR-4/blob/main/14.png)
#### Рис. 14 - Графік алгоритму №4c

### Аналіз та контрольні питання:
#### Питання 1: Порівняйте кількість ітерацій, яка знадобилася для збіжності у кожному експерименті.
#### Питання 2: Чи вплинуло початкове припущення на фінальну точність? Чому так (або чому ні)? (Підказка: подумайте про форму "чаші" нашої функції втрат).
#### Відповідь 1: Від [50000,50000] 433 ітерацій, від [0,0] 367, від [-500000,-500000] 51126.
#### Відповідь 2: Ні, фінальна точність однакова (за однакового шуму). Функція втрат має форму "чаші" з одним глобальним мінімумом. Початкова точка впливає лише на швидкість, не на результат.

### 5) Дослідження Геометрії ("Прокляття плаского каньйону" - GDOP)
#### Встановлюємо наступні налаштування:
``Експеримент 5a (Хороша геометрія): TRUE_POSITION = [45000, 35000] (ціль всередині трикутника станцій).
Експеримент 5b (Погана геометрія): TRUE_POSITION = [45000, -135000] (ціль далеко збоку від усіх станцій).``

#### Після виставлення налаштувань запускаємо алгоритм та дивимось на результат, потім послідовно змінюємо TRUE_POSITION:

![15](https://github.com/GAMECHl/LB-PR-4/blob/main/15.png)
#### Рис. 15 - Графік алгоритму №5а

![16](https://github.com/GAMECHl/LB-PR-4/blob/main/16.png)
#### Рис. 16 - Графік алгоритму №5b

### Аналіз та контрольні питання:
#### Питання 1: Порівняйте фінальну похибку позиціонування для 5a та 5b. Рівень шуму був однаковий (1e-6). Чому результати відрізняються на порядки (десятки метрів проти кілометрів)?
#### Питання 2: Проаналізуйте графік "Delta Loss" для експерименту 5b. Чому алгоритм зупинився, хоча похибка позиціонування була ще величезною? Поясніть аналогію з "пласким дном каньйону".
#### Відповідь 1: У 5a похибка 343 метра, у 5b - 4301 метрів. Погана геометрія (ціль на одній лінії зі станціями) робить систему чутливою до шуму. Малі помилки в TDoA → величезні похибки в позиції.
#### Відповідь 2: Delta Loss стало < TOLERANCE. Алгоритм застряг на плоскому дні — градієнт майже нульовий, хоч ми далеко від істини. Функція втрат погано обумовлена через геометрію.

### 6) Дослідження TOLERANCE ("Ціна точності")
#### Встановлюємо наступні налаштування:
`` Використовуємо "погану" геометрію з Завдання 5b (TRUE_POSITION = [45000, -135000]).
Експеримент 6a (Швидко і брудно): TOLERANCE = 1e-2
Експеримент 6b (Базовий): TOLERANCE = 1e-7
Експеримент 6c (Надточний): TOLERANCE = 1e-12 (також збільште MAX_ITERATIONS до 100000, щоб дати йому час)``

#### Після виставлення налаштувань запускаємо алгоритм та дивимось на результат, потім послідовно змінюємо TOLERANCE:

![17](https://github.com/GAMECHl/LB-PR-4/blob/main/17.png)
#### Рис. 17 - Графік алгоритму №6а

![18](https://github.com/GAMECHl/LB-PR-4/blob/main/18.png)
#### Рис. 18 - Графік алгоритму №6b

![19](https://github.com/GAMECHl/LB-PR-4/blob/main/19.png)
#### Рис. 19 - Графік алгоритму №6c

### Аналіз та контрольні питання:
#### Питання 1: Порівняйте тріо: К-сть ітерацій | Фінальна Похибка | Фінальне значення Loss.
#### Питання 2: Чи варто було "платити" додаткові 50,000 ітерацій за перехід від 1e-7 до 1e-12? Поясніть, чому TOLERANCE не може виправити фундаментальні проблеми (шум та погану геометрію).
#### Відповідь 1: Таблиця:
![20](https://github.com/GAMECHl/LB-PR-4/blob/main/20.png)
#### Відповідь 2: Ні, не варто. TOLERANCE контролює лише точність оптимізації, але не може виправити погані вхідні дані (шум) або фундаментальні проблеми (GDOP).

## Загальний висновок: Найбільший вплив мають: 1) Якість даних (шум), 2) Геометрія системи (GDOP), 3) Learning rate. Початкове припущення та tolerance — вторинні фактори. 

